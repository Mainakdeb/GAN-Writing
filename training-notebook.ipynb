{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torchvision.datasets as datasets\nimport string\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\nfrom torchvision.datasets.utils import download_and_extract_archive, extract_archive, verify_str_arg, check_integrity\nimport shutil\n\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\n#https://pytorch.org/vision/stable/_modules/torchvision/datasets/mnist.html#EMNIST\n\nclass EMNIST(datasets.MNIST):\n    url = 'https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip'\n    md5 = \"58c8d27c78d21e728a6bc7b3cc06412e\"\n    splits = ('byclass', 'bymerge', 'balanced', 'letters', 'digits', 'mnist')\n    # Merged Classes assumes Same structure for both uppercase and lowercase version\n    _merged_classes = {'c', 'i', 'j', 'k', 'l', 'm', 'o', 'p', 's', 'u', 'v', 'w', 'x', 'y', 'z'}\n    _all_classes = set(string.ascii_lowercase+string.ascii_uppercase)\n    classes_split_dict = {\n        'byclass': sorted(list(_all_classes)),\n        'bymerge': sorted(list(_all_classes - _merged_classes)),\n        'balanced': sorted(list(_all_classes - _merged_classes)),\n        'letters': ['N/A'] + list(string.ascii_lowercase),\n        'digits': list(string.digits),\n        'mnist': list(string.digits),\n    }\n\n    def __init__(self, root: str, split: str, **kwargs: Any) -> None:\n        self.split = verify_str_arg(split, \"split\", self.splits)\n        self.training_file = self._training_file(split)\n        self.test_file = self._test_file(split)\n        super(EMNIST, self).__init__(root, **kwargs)\n        self.classes = self.classes_split_dict[self.split]\n\n    @staticmethod\n    def _training_file(split) -> str:\n        return 'training_{}.pt'.format(split)\n\n    @staticmethod\n    def _test_file(split) -> str:\n        return 'test_{}.pt'.format(split)\n\n    @property\n    def _file_prefix(self) -> str:\n        return f\"emnist-{self.split}-{'train' if self.train else 'test'}\"\n\n    @property\n    def images_file(self) -> str:\n        return os.path.join(self.raw_folder, f\"{self._file_prefix}-images-idx3-ubyte\")\n\n    @property\n    def labels_file(self) -> str:\n        return os.path.join(self.raw_folder, f\"{self._file_prefix}-labels-idx1-ubyte\")\n\n    def _load_data(self):\n        return read_image_file(self.images_file), read_label_file(self.labels_file)\n\n    def _check_exists(self) -> bool:\n        return all(check_integrity(file) for file in (self.images_file, self.labels_file))\n\n    def download(self) -> None:\n        \"\"\"Download the EMNIST data if it doesn't exist already.\"\"\"\n\n        if self._check_exists():\n            return\n\n        os.makedirs(self.raw_folder, exist_ok=True)\n\n        download_and_extract_archive(self.url, download_root=self.raw_folder, md5=self.md5)\n        gzip_folder = os.path.join(self.raw_folder, 'gzip')\n        for gzip_file in os.listdir(gzip_folder):\n            if gzip_file.endswith('.gz'):\n                extract_archive(os.path.join(gzip_folder, gzip_file), self.raw_folder)\n        shutil.rmtree(gzip_folder)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as tran\nimport torchvision.utils as vutils\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nfrom IPython.display import clear_output\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nworkers = 2\nbatch_size = 64\n#image_size = 64\nnc = 1 # Number of channels in the training images\nnz = 620 # Size of z latent vector\nngf = 128 # Size of feature maps in generator\nndf = 128 # Size of feature maps in discriminator\nlr = 4e-4 # Learning rate for optimizers\nbeta1 = 0.5 # eBta1 hyperparam for Adam optimizers\nngpu = 1\n\ntransforms = tran.Compose(\n    [\n     #tran.Resize(image_size),\n     tran.RandomRotation(15),\n     #tran.CenterCrop(25),\n     #tran.RandomPerspective(distortion_scale=0.15, p=0.6),\n     #tran.RandomAffine(degrees=3, translate=None, scale=None, shear=2),\n     lambda img: tran.functional.rotate(img, angle=-90),\n     lambda img: tran.functional.hflip(img),\n     tran.ToTensor(),\n     tran.Normalize(\n         [0.5 for _ in range(nc)], \n         [0.5 for _ in range(nc)]\n         )\n    ]\n)\n\n#dataset and dataloader\ndataset = datasets.EMNIST(root=\"/dataset/\", split=\"balanced\", train=True, transform=transforms, download=True)\n\n# inds = list(range(0, len(dataset),3))\n# set_1 = torch.utils.data.Subset(dataset, inds)\ndataloader = DataLoader(dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idx = (dataset.targets==26) | (dataset.targets==27) | (dataset.targets==28) | (dataset.targets==29) | (dataset.targets==30) | (dataset.targets==31) | (dataset.targets==32) | (dataset.targets==33) | (dataset.targets==34) | (dataset.targets==35) | (dataset.targets==36) | (dataset.targets==37) | (dataset.targets==38) | (dataset.targets==39) | (dataset.targets==40) | (dataset.targets==41) |  (dataset.targets==42) |  (dataset.targets==43) |  (dataset.targets==44) |  (dataset.targets==45) |  (dataset.targets==46) |  (dataset.targets==47) |  (dataset.targets==48) |  (dataset.targets==49) |  (dataset.targets==50) |  (dataset.targets==51) |  (dataset.targets==52)\n# dataset.targets = dataset.targets[idx]\n# dataset.data = dataset.data[idx]\n# dataset.classes = dataset.classes[idx]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = dataset.classes\nl.sort()\nprint(\"No of classes: \",len(l))\nprint(\"List of all classes\")\nprint(l)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import make_grid\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(20, 20))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=20).permute(1, 2, 0))\n        break\n        \nshow_batch(dataloader)\n\n#inconsistency bw dataloader and dataset class","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes_num = 47\n\nclass Generator(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n\n        self.label_emb = nn.Embedding(classes_num, classes_num)\n\n        self.tconv1 = nn.ConvTranspose2d(nz + classes_num, ngf * 8, kernel_size=3, stride=1, padding=0, bias=False)\n        self.bn1 = nn.BatchNorm2d(ngf * 8)\n\n        self.tconv2 = nn.ConvTranspose2d(ngf * 8, ngf * 8, 4, 2, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(ngf * 8)\n\n        self.tconv3 = nn.ConvTranspose2d(ngf * 8, ngf * 2, 4, 2, 0, bias=False)\n        self.bn3 = nn.BatchNorm2d(ngf * 2)\n\n        self.tconv4 = nn.ConvTranspose2d(ngf*2, nc, 4, 2, 1, bias=False)\n\n    def forward(self, x, labels):\n        c = self.label_emb(labels)\n        c = c.unsqueeze(2).unsqueeze(3)\n\n        # print(c.size())\n        # print(x.size())\n\n        x = torch.cat([x, c], 1)\n        x = F.relu(self.bn1(self.tconv1(x)))\n        x = F.relu(self.bn2(self.tconv2(x)))\n        x = F.relu(self.bn3(self.tconv3(x)))\n\n        # x = F.relu(self.bn5(self.tconv5(x)))\n        x = torch.tanh(self.tconv4(x))\n\n        return x\n\n\nnetG = Generator(ngpu).to(device)\n\nclass Discriminator(nn.Module):\n    def __init__(self, params):\n        super().__init__()\n\n        # meta data (label)\n        self.label_emb = nn.Embedding(classes_num, classes_num)\n\n        self.conv1 = nn.Conv2d(nc, ndf*2, 5, 2, 1, bias=False)\n\n        self.conv2 = nn.Conv2d(ndf*2, ndf * 2, 5, 2, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(ndf * 2)\n\n        self.conv3 = nn.Conv2d(ndf * 2, classes_num, 5, 2, 0, bias=False)\n\n\n        self.fc1 = nn.Linear(classes_num*2, classes_num)\n        self.fc2 = nn.Linear(classes_num, 1)\n\n    def forward(self, x, labels):\n        x = F.leaky_relu(self.conv1(x), 0.2, True)\n        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, True)\n        x = F.leaky_relu(self.conv3(x))\n        x = torch.flatten(x, 1)\n\n        c = self.label_emb(labels)\n        x = torch.cat([x, c], 1)\n        x = F.leaky_relu(self.fc1(x))\n        x = F.sigmoid(self.fc2(x))\n\n        return x\n\nnetD = Discriminator(ngpu).to(device)\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\nnetD.apply(weights_init)\nnetG.apply(weights_init)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# netD.load_state_dict(torch.load(\"../input/modelsgw3/discriminator_gw_3.pth\"))\n# netG.load_state_dict(torch.load(\"../input/modelsgw3/generator_gw_3.pth\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test gen\nnoise = torch.randn(10, nz, 1, 1, device=device).to(device)\nfake_labels = Variable(torch.LongTensor(np.random.randint(0, classes_num, 10))).to(device)\nfake = netG(noise, fake_labels)\ns=fake.detach().cpu().numpy()\nplt.imshow(s[0][0])\nprint(s[0][0].shape)\n\n#test disc\nfake_labels = Variable(torch.LongTensor(np.random.randint(0, classes_num, batch_size))).to(device)\nnoise2 = torch.randn(batch_size, nz, 1, 1, device=device)\nfake = netG(noise2, fake_labels)\noutput = netD(fake, fake_labels).view(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\nreal_label = 1.\nfake_label = 0.\n\ncriterion = nn.BCELoss()\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=0.001)\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\nimg_list = []\nG_losses = []\nD_losses = []\nD_xs = []\niters = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"netG.load_state_dict(torch.load(\"../input/gwbase3/generator_gw_8.pth\"))\nnetD.load_state_dict(torch.load(\"../input/gwbase3/discriminator_gw_8.pth\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs =2\n\nfor epoch in range(num_epochs):\n    for i, data in enumerate(dataloader, 0):\n        clear_output(wait=True)\n        \n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        #print((data[1]))\n        netD.zero_grad()\n        # Format batch\n#         print(\"debug\", data[1])\n        real_cpu = data[0].to(device)\n        real_labels = data[1].to(device)\n        real_labels=real_labels\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        # Forward pass real batch through D\n        output = netD(real_cpu, real_labels).view(-1)\n        # Calculate loss on all-real batch\n        # print(output[0])\n        # print(label[0])\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        fake_labels = Variable(torch.LongTensor(np.random.randint(0, classes_num, batch_size))).to(device)\n        # Generate fake image batch with G\n        fake = netG(noise, fake_labels)\n        label.fill_(fake_label)\n        # Classify all fake batch with D\n        output = netD(fake.detach(), fake_labels).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # Add the gradients from the all-real and all-fake batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake, fake_labels).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n        \n        # print training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n            \n            plt.figure(figsize=(10,5))\n            plt.title(\"Generator and Discriminator Loss\")\n            plt.plot(G_losses,label=\"Gen\")\n            plt.plot(D_losses,label=\"Disc\")\n            plt.xlabel(\"iterations\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            plt.show()\n\n            plt.figure(figsize=(10,5))\n            plt.title(\"D(x) \")\n            plt.plot(D_xs)\n            plt.xlabel(\"iterations\")\n            plt.ylabel(\"D(x)\")\n            plt.show()\n    \n        \n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n        D_xs.append(D_x)\n        \n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise, fake_labels).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n        iters += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_digit_from_label(label, seed):\n  fake_label = torch.tensor([label]).cuda()\n  with torch.no_grad():\n    fake_ = netG(seed, fake_label).detach().cpu()\n  return(fake_.squeeze())\n\n\nfig, axes = plt.subplots(1,9, figsize = (20,8))\nseed = torch.randn(1, nz, 1, 1, device=device)\n\ndef generate_noisy_seed(seed):\n    seed = seed + (0.2**0.4)*torch.randn(1, nz, 1, 1, device=device)\n    return(seed)\n    \n# seed = torch.randn(1, nz, 1, 1, device=device)\n\n# axes[0].set_title('label 0')\naxes[0].imshow(1-generate_digit_from_label(16, generate_noisy_seed(seed)), cmap=\"gray\")\naxes[0].axis('off')\n# axes[1].set_title('label 1')\naxes[1].imshow(1-generate_digit_from_label(14, generate_noisy_seed(seed)), cmap=\"gray\")\naxes[1].axis('off')\n# axes[2].set_title('label 2')\naxes[2].imshow(1-generate_digit_from_label(23, generate_noisy_seed(seed)), cmap=\"gray\")\naxes[2].axis('off')\n# axes[3].set_title('label 3')\naxes[3].imshow(1-generate_digit_from_label(14, generate_noisy_seed(seed)), cmap=\"gray\")\naxes[3].axis('off')\n# axes[4].set_title('label 4')\naxes[4].imshow(1-generate_digit_from_label(27, generate_noisy_seed(seed)), cmap=\"gray\")\naxes[4].axis('off')\n# axes[5].set_title('label 5')\naxes[5].imshow(1-generate_digit_from_label(10, generate_noisy_seed(seed)), cmap=\"gray\")\naxes[5].axis('off')\n# axes[6].set_title('label 6')\naxes[6].imshow(1-generate_digit_from_label(29, generate_noisy_seed(seed)), cmap=\"gray\")\naxes[6].axis('off')\n# axes[7].set_title('label 7')\naxes[7].imshow(1-generate_digit_from_label(14, generate_noisy_seed(seed)), cmap=\"gray\")\naxes[7].axis('off')\n# axes[8].set_title('label 8')\naxes[8].imshow(1-generate_digit_from_label(13, generate_noisy_seed(seed)), cmap=\"gray\")\naxes[8].axis('off')\n# axes[9].set_title('label 9')\n# axes[9].imshow(1-generate_digit_from_label(26, seed), cmap=\"gray\")","metadata":{"execution":{"iopub.status.busy":"2021-09-30T18:48:49.973683Z","iopub.execute_input":"2021-09-30T18:48:49.973966Z","iopub.status.idle":"2021-09-30T18:48:50.395745Z","shell.execute_reply.started":"2021-09-30T18:48:49.973918Z","shell.execute_reply":"2021-09-30T18:48:50.395075Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"for i in range(classes_num):\n    plt.imshow(1-generate_digit_from_label(i, seed), cmap='gray')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(netG.state_dict(), \"generator_gw_8.pth\")\ntorch.save(netD.state_dict(), \"discriminator_gw_8.pth\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'./discriminator_gw_8.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(r'./discriminator_gw_8.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_and_save_image_with_multiple_digits(digits, seed):\n  gen_list=[]\n  for dig in str(digits):\n    gen_list.append(generate_digit_from_label(int(dig), seed))\n  vis = np.concatenate((gen_list), axis=1)\n  vis = 255-vis\n  mimg.imsave(\"generated\"+str(digits)+\".png\", vis, cmap='gray')\n  return(vis)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'D', 'E', 'F', 'G', 'H', 'N', 'Q', 'R', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']","metadata":{}},{"cell_type":"code","source":"plt.imshow(generate_and_save_image_with_multiple_digits(12110, seed), cmap='gray')\nplt.axis(\"off\")\nplt.show()\nplt.imshow(generate_and_save_image_with_multiple_digits(410, seed), cmap='gray')\nplt.axis(\"off\")\nplt.show()\nplt.imshow(generate_and_save_image_with_multiple_digits(6789, seed), cmap='gray')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}